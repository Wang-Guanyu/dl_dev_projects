{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 1: Addition Basic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from random import shuffle\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n",
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_of_class_x = 101 # Number of classes of 1st input\n",
    "num_of_class_y = 101 # Number of classes of 2nd input\n",
    "num_classes = 201 # Number of classes for output result\n",
    "\n",
    "num_data = 100000\n",
    "remove_list = [25,50,75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    t = [] # Normal data and label set\n",
    "    v = [] # Removed data and label set\n",
    "    for i in range(num_data):\n",
    "        d1 = np.random.randint(0,num_of_class_x)\n",
    "        d2 = np.random.randint(0,num_of_class_y)\n",
    "        if d1 in remove_list or d2 in remove_list:\n",
    "            v.append((d1,d2,d1+d2))\n",
    "            continue\n",
    "        t.append((d1,d2,d1+d2))\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------- x_data ---------------\n",
      "\n",
      "[ 53.  88.]\n",
      "\n",
      "--------- normalized_x_data ---------\n",
      "\n",
      "[ 0.52999997  0.88      ]\n",
      "\n",
      "-------------- y_data ---------------\n",
      "\n",
      "141.0\n",
      "\n",
      "---------- one_hot_y_data -----------\n",
      "\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "z = generate_data()\n",
    "\n",
    "# Split into data & label\n",
    "x = np.array([v[0:2] for v in z]).astype('float32')\n",
    "y = np.array([v[2:][0] for v in z]).astype('float32')\n",
    "\n",
    "x_data = x / (num_of_class_x - 1) # Normalize data to [0,1]\n",
    "y_data = to_categorical(y,num_classes) # One hot encoding\n",
    "\n",
    "# Split into training set and testing set\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data,y_data,test_size = 0.2)\n",
    "\n",
    "print('\\n-------------- x_data ---------------\\n')\n",
    "print(x[100])\n",
    "print('\\n--------- normalized_x_data ---------\\n')\n",
    "print(x_data[100])\n",
    "print('\\n-------------- y_data ---------------\\n')\n",
    "print(y[100])\n",
    "print('\\n---------- one_hot_y_data -----------\\n')\n",
    "print(y_data[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 400)               1200      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               120300    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 201)               20301     \n",
      "=================================================================\n",
      "Total params: 222,101\n",
      "Trainable params: 222,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_data = Input(shape=(2,))\n",
    "\n",
    "x = Dense(400,activation='relu',name='dense_1')(input_data)\n",
    "x = Dense(300,activation='relu',name='dense_2')(x)\n",
    "x = Dense(200,activation='relu',name='dense_3')(x)\n",
    "x = Dense(100,activation='relu',name='dense_4')(x)\n",
    "output_result = Dense(num_classes,activation='softmax',name='out')(x)\n",
    "\n",
    "model = Model(input_data,output_result)\n",
    "\n",
    "# Define loss function & optimizer\n",
    "adam = keras.optimizers.Adam(lr=0.005,beta_1=0.9,beta_2=0.999,epsilon=1e-08,decay=0.01)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75371 samples, validate on 18843 samples\n",
      "Epoch 1/300\n",
      "75371/75371 [==============================] - 1s 15us/step - loss: 5.2110 - acc: 0.0091 - val_loss: 5.0848 - val_acc: 0.0096\n",
      "Epoch 2/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 5.0274 - acc: 0.0089 - val_loss: 4.9451 - val_acc: 0.0089\n",
      "Epoch 3/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 4.8514 - acc: 0.0111 - val_loss: 4.6473 - val_acc: 0.0195\n",
      "Epoch 4/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 4.4042 - acc: 0.0284 - val_loss: 4.0193 - val_acc: 0.0568\n",
      "Epoch 5/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 3.8834 - acc: 0.0469 - val_loss: 3.6510 - val_acc: 0.0734\n",
      "Epoch 6/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 3.5277 - acc: 0.0792 - val_loss: 3.3624 - val_acc: 0.0881\n",
      "Epoch 7/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 3.2652 - acc: 0.1022 - val_loss: 3.1472 - val_acc: 0.1092\n",
      "Epoch 8/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 3.0491 - acc: 0.1340 - val_loss: 2.9667 - val_acc: 0.1394\n",
      "Epoch 9/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 2.8918 - acc: 0.1421 - val_loss: 2.8712 - val_acc: 0.1178\n",
      "Epoch 10/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 2.7758 - acc: 0.1459 - val_loss: 2.6733 - val_acc: 0.1919\n",
      "Epoch 11/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 2.6499 - acc: 0.1850 - val_loss: 2.6123 - val_acc: 0.1613\n",
      "Epoch 12/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 2.5890 - acc: 0.1790 - val_loss: 2.5367 - val_acc: 0.1882\n",
      "Epoch 13/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 2.5143 - acc: 0.1964 - val_loss: 2.4590 - val_acc: 0.2033\n",
      "Epoch 14/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 2.4366 - acc: 0.2153 - val_loss: 2.4109 - val_acc: 0.2365\n",
      "Epoch 15/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 2.3722 - acc: 0.2362 - val_loss: 2.3418 - val_acc: 0.2428\n",
      "Epoch 16/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 2.3132 - acc: 0.2523 - val_loss: 2.3402 - val_acc: 0.2128\n",
      "Epoch 17/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 2.3701 - acc: 0.1919 - val_loss: 2.2502 - val_acc: 0.2709\n",
      "Epoch 18/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 2.2636 - acc: 0.2414 - val_loss: 2.2810 - val_acc: 0.1957\n",
      "Epoch 19/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 2.2125 - acc: 0.2607 - val_loss: 2.1783 - val_acc: 0.2949\n",
      "Epoch 20/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 2.1512 - acc: 0.3009 - val_loss: 2.1311 - val_acc: 0.3099\n",
      "Epoch 21/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 2.1147 - acc: 0.2964 - val_loss: 2.1008 - val_acc: 0.3147\n",
      "Epoch 22/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 2.0786 - acc: 0.3218 - val_loss: 2.0656 - val_acc: 0.3373\n",
      "Epoch 23/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 2.0441 - acc: 0.3359 - val_loss: 2.0493 - val_acc: 0.2925\n",
      "Epoch 24/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 2.0378 - acc: 0.3114 - val_loss: 2.0246 - val_acc: 0.3434\n",
      "Epoch 25/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 2.0017 - acc: 0.3362 - val_loss: 1.9963 - val_acc: 0.3554\n",
      "Epoch 26/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.9820 - acc: 0.3456 - val_loss: 2.0111 - val_acc: 0.3094\n",
      "Epoch 27/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.9581 - acc: 0.3347 - val_loss: 1.9711 - val_acc: 0.3569\n",
      "Epoch 28/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.9365 - acc: 0.3600 - val_loss: 1.9357 - val_acc: 0.3663\n",
      "Epoch 29/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.9096 - acc: 0.3516 - val_loss: 1.9185 - val_acc: 0.3340\n",
      "Epoch 30/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.8791 - acc: 0.3732 - val_loss: 1.9026 - val_acc: 0.3762\n",
      "Epoch 31/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.8592 - acc: 0.3810 - val_loss: 1.8619 - val_acc: 0.3703\n",
      "Epoch 32/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.8405 - acc: 0.3862 - val_loss: 1.8274 - val_acc: 0.4170\n",
      "Epoch 33/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.8015 - acc: 0.4093 - val_loss: 1.8226 - val_acc: 0.4122\n",
      "Epoch 34/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.8113 - acc: 0.3923 - val_loss: 1.7942 - val_acc: 0.3665\n",
      "Epoch 35/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.7854 - acc: 0.3799 - val_loss: 1.7775 - val_acc: 0.4022\n",
      "Epoch 36/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.7572 - acc: 0.4149 - val_loss: 1.7731 - val_acc: 0.3702\n",
      "Epoch 37/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.7578 - acc: 0.3925 - val_loss: 1.7514 - val_acc: 0.4156\n",
      "Epoch 38/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.7278 - acc: 0.4337 - val_loss: 1.7258 - val_acc: 0.4393\n",
      "Epoch 39/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.7126 - acc: 0.4505 - val_loss: 1.7256 - val_acc: 0.4441\n",
      "Epoch 40/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.6901 - acc: 0.4627 - val_loss: 1.6969 - val_acc: 0.4350\n",
      "Epoch 41/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.6873 - acc: 0.4403 - val_loss: 1.6972 - val_acc: 0.4674\n",
      "Epoch 42/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.6662 - acc: 0.4478 - val_loss: 1.6627 - val_acc: 0.4997\n",
      "Epoch 43/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.6493 - acc: 0.4513 - val_loss: 1.6610 - val_acc: 0.4466\n",
      "Epoch 44/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.6456 - acc: 0.4607 - val_loss: 1.6772 - val_acc: 0.4379\n",
      "Epoch 45/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.6311 - acc: 0.4600 - val_loss: 1.6393 - val_acc: 0.4601\n",
      "Epoch 46/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.6136 - acc: 0.4644 - val_loss: 1.6333 - val_acc: 0.4493\n",
      "Epoch 47/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.6037 - acc: 0.4759 - val_loss: 1.6026 - val_acc: 0.5059\n",
      "Epoch 48/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.5898 - acc: 0.4849 - val_loss: 1.6299 - val_acc: 0.4167\n",
      "Epoch 49/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.5906 - acc: 0.4659 - val_loss: 1.6120 - val_acc: 0.4596\n",
      "Epoch 50/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.5759 - acc: 0.4810 - val_loss: 1.5871 - val_acc: 0.5079\n",
      "Epoch 51/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.5534 - acc: 0.5077 - val_loss: 1.5753 - val_acc: 0.4698\n",
      "Epoch 52/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.5523 - acc: 0.4865 - val_loss: 1.5464 - val_acc: 0.4804\n",
      "Epoch 53/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.5302 - acc: 0.5155 - val_loss: 1.5480 - val_acc: 0.4775\n",
      "Epoch 54/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.5339 - acc: 0.4897 - val_loss: 1.5404 - val_acc: 0.4928\n",
      "Epoch 55/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.5207 - acc: 0.4990 - val_loss: 1.5370 - val_acc: 0.4863\n",
      "Epoch 56/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.5062 - acc: 0.5121 - val_loss: 1.5227 - val_acc: 0.5121\n",
      "Epoch 57/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.4907 - acc: 0.5322 - val_loss: 1.4989 - val_acc: 0.5541\n",
      "Epoch 58/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.4816 - acc: 0.5318 - val_loss: 1.4887 - val_acc: 0.5605\n",
      "Epoch 59/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.4790 - acc: 0.5426 - val_loss: 1.5079 - val_acc: 0.4775\n",
      "Epoch 60/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.4714 - acc: 0.5331 - val_loss: 1.4724 - val_acc: 0.5569\n",
      "Epoch 61/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.4605 - acc: 0.5670 - val_loss: 1.4617 - val_acc: 0.5692\n",
      "Epoch 62/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.4417 - acc: 0.5704 - val_loss: 1.4635 - val_acc: 0.5752\n",
      "Epoch 63/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.4348 - acc: 0.5673 - val_loss: 1.4375 - val_acc: 0.5691\n",
      "Epoch 64/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.4272 - acc: 0.5759 - val_loss: 1.4471 - val_acc: 0.5382\n",
      "Epoch 65/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.4198 - acc: 0.5603 - val_loss: 1.4337 - val_acc: 0.5313\n",
      "Epoch 66/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.4113 - acc: 0.5631 - val_loss: 1.4074 - val_acc: 0.5761\n",
      "Epoch 67/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.3956 - acc: 0.5829 - val_loss: 1.4187 - val_acc: 0.5622\n",
      "Epoch 68/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.3921 - acc: 0.5888 - val_loss: 1.3914 - val_acc: 0.5764\n",
      "Epoch 69/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.3758 - acc: 0.6142 - val_loss: 1.3949 - val_acc: 0.5846\n",
      "Epoch 70/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.3699 - acc: 0.6132 - val_loss: 1.3774 - val_acc: 0.5909\n",
      "Epoch 71/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.3656 - acc: 0.6078 - val_loss: 1.3750 - val_acc: 0.5958\n",
      "Epoch 72/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.3620 - acc: 0.5901 - val_loss: 1.3704 - val_acc: 0.5795\n",
      "Epoch 73/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.3512 - acc: 0.6058 - val_loss: 1.3684 - val_acc: 0.5832\n",
      "Epoch 74/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.3508 - acc: 0.5917 - val_loss: 1.3529 - val_acc: 0.5876\n",
      "Epoch 75/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.3459 - acc: 0.5835 - val_loss: 1.3587 - val_acc: 0.5706\n",
      "Epoch 76/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.3410 - acc: 0.5950 - val_loss: 1.3629 - val_acc: 0.5836\n",
      "Epoch 77/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.3357 - acc: 0.5918 - val_loss: 1.3302 - val_acc: 0.5937\n",
      "Epoch 78/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.3292 - acc: 0.5982 - val_loss: 1.3411 - val_acc: 0.5908\n",
      "Epoch 79/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.3176 - acc: 0.6137 - val_loss: 1.3207 - val_acc: 0.6116\n",
      "Epoch 80/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.3101 - acc: 0.6097 - val_loss: 1.3211 - val_acc: 0.6066\n",
      "Epoch 81/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.3024 - acc: 0.6113 - val_loss: 1.3268 - val_acc: 0.5735\n",
      "Epoch 82/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.2974 - acc: 0.6154 - val_loss: 1.3072 - val_acc: 0.6001\n",
      "Epoch 83/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.2965 - acc: 0.6032 - val_loss: 1.3072 - val_acc: 0.5928\n",
      "Epoch 84/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.2838 - acc: 0.6390 - val_loss: 1.3092 - val_acc: 0.6395\n",
      "Epoch 85/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.2865 - acc: 0.6299 - val_loss: 1.3070 - val_acc: 0.6075\n",
      "Epoch 86/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.2791 - acc: 0.6236 - val_loss: 1.2971 - val_acc: 0.5953\n",
      "Epoch 87/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.2710 - acc: 0.6336 - val_loss: 1.2747 - val_acc: 0.6530\n",
      "Epoch 88/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.2615 - acc: 0.6648 - val_loss: 1.2859 - val_acc: 0.6237\n",
      "Epoch 89/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.2597 - acc: 0.6431 - val_loss: 1.2703 - val_acc: 0.6260\n",
      "Epoch 90/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.2518 - acc: 0.6409 - val_loss: 1.2671 - val_acc: 0.6514\n",
      "Epoch 91/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.2473 - acc: 0.6549 - val_loss: 1.2619 - val_acc: 0.6293\n",
      "Epoch 92/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.2390 - acc: 0.6529 - val_loss: 1.2624 - val_acc: 0.5996\n",
      "Epoch 93/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.2369 - acc: 0.6364 - val_loss: 1.2431 - val_acc: 0.6751\n",
      "Epoch 94/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.2334 - acc: 0.6456 - val_loss: 1.2408 - val_acc: 0.6511\n",
      "Epoch 95/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.2200 - acc: 0.6592 - val_loss: 1.2354 - val_acc: 0.6618\n",
      "Epoch 96/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.2183 - acc: 0.6568 - val_loss: 1.2263 - val_acc: 0.6775\n",
      "Epoch 97/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.2141 - acc: 0.6838 - val_loss: 1.2245 - val_acc: 0.6564\n",
      "Epoch 98/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.2062 - acc: 0.6766 - val_loss: 1.2271 - val_acc: 0.6636\n",
      "Epoch 99/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.2017 - acc: 0.6806 - val_loss: 1.2184 - val_acc: 0.6975\n",
      "Epoch 100/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.1982 - acc: 0.6715 - val_loss: 1.2072 - val_acc: 0.6637\n",
      "Epoch 101/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.1923 - acc: 0.6751 - val_loss: 1.2040 - val_acc: 0.7034\n",
      "Epoch 102/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.1952 - acc: 0.6668 - val_loss: 1.2014 - val_acc: 0.6591\n",
      "Epoch 103/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.1905 - acc: 0.6582 - val_loss: 1.2075 - val_acc: 0.6727\n",
      "Epoch 104/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.1827 - acc: 0.6765 - val_loss: 1.1977 - val_acc: 0.6766\n",
      "Epoch 105/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.1744 - acc: 0.6929 - val_loss: 1.1903 - val_acc: 0.6831\n",
      "Epoch 106/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.1738 - acc: 0.6819 - val_loss: 1.1842 - val_acc: 0.7088\n",
      "Epoch 107/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.1758 - acc: 0.6762 - val_loss: 1.1931 - val_acc: 0.6311\n",
      "Epoch 108/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.1706 - acc: 0.6850 - val_loss: 1.1817 - val_acc: 0.6703\n",
      "Epoch 109/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.1629 - acc: 0.6763 - val_loss: 1.1663 - val_acc: 0.6647\n",
      "Epoch 110/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.1516 - acc: 0.6948 - val_loss: 1.1742 - val_acc: 0.6198\n",
      "Epoch 111/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.1497 - acc: 0.6897 - val_loss: 1.1697 - val_acc: 0.7071\n",
      "Epoch 112/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.1493 - acc: 0.6940 - val_loss: 1.1567 - val_acc: 0.6888\n",
      "Epoch 113/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.1399 - acc: 0.6991 - val_loss: 1.1554 - val_acc: 0.7176\n",
      "Epoch 114/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.1410 - acc: 0.6987 - val_loss: 1.1501 - val_acc: 0.7090\n",
      "Epoch 115/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.1371 - acc: 0.7074 - val_loss: 1.1418 - val_acc: 0.7244\n",
      "Epoch 116/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.1293 - acc: 0.7054 - val_loss: 1.1479 - val_acc: 0.7097\n",
      "Epoch 117/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.1277 - acc: 0.7091 - val_loss: 1.1412 - val_acc: 0.6919\n",
      "Epoch 118/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.1215 - acc: 0.7129 - val_loss: 1.1309 - val_acc: 0.7477\n",
      "Epoch 119/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.1148 - acc: 0.7245 - val_loss: 1.1360 - val_acc: 0.7062\n",
      "Epoch 120/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.1132 - acc: 0.7263 - val_loss: 1.1358 - val_acc: 0.6930\n",
      "Epoch 121/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.1123 - acc: 0.7223 - val_loss: 1.1246 - val_acc: 0.6900\n",
      "Epoch 122/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.1068 - acc: 0.7064 - val_loss: 1.1140 - val_acc: 0.7038\n",
      "Epoch 123/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.1023 - acc: 0.7255 - val_loss: 1.1135 - val_acc: 0.7371\n",
      "Epoch 124/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.1002 - acc: 0.7182 - val_loss: 1.1143 - val_acc: 0.6942\n",
      "Epoch 125/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0960 - acc: 0.7096 - val_loss: 1.1133 - val_acc: 0.7025\n",
      "Epoch 126/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0950 - acc: 0.7152 - val_loss: 1.1172 - val_acc: 0.7117\n",
      "Epoch 127/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0939 - acc: 0.7108 - val_loss: 1.1084 - val_acc: 0.7081\n",
      "Epoch 128/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0900 - acc: 0.7022 - val_loss: 1.0926 - val_acc: 0.7220\n",
      "Epoch 129/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0799 - acc: 0.7319 - val_loss: 1.0971 - val_acc: 0.7258\n",
      "Epoch 130/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0827 - acc: 0.7168 - val_loss: 1.0949 - val_acc: 0.7082\n",
      "Epoch 131/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0824 - acc: 0.7108 - val_loss: 1.1028 - val_acc: 0.6890\n",
      "Epoch 132/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0752 - acc: 0.7121 - val_loss: 1.0872 - val_acc: 0.7507\n",
      "Epoch 133/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0698 - acc: 0.7431 - val_loss: 1.0878 - val_acc: 0.7224\n",
      "Epoch 134/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0638 - acc: 0.7431 - val_loss: 1.0824 - val_acc: 0.7076\n",
      "Epoch 135/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0650 - acc: 0.7327 - val_loss: 1.0836 - val_acc: 0.7290\n",
      "Epoch 136/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0612 - acc: 0.7301 - val_loss: 1.0689 - val_acc: 0.7397\n",
      "Epoch 137/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0581 - acc: 0.7280 - val_loss: 1.0646 - val_acc: 0.7735\n",
      "Epoch 138/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0503 - acc: 0.7517 - val_loss: 1.0682 - val_acc: 0.7508\n",
      "Epoch 139/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0507 - acc: 0.7335 - val_loss: 1.0637 - val_acc: 0.7759\n",
      "Epoch 140/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0471 - acc: 0.7437 - val_loss: 1.0624 - val_acc: 0.7521\n",
      "Epoch 141/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0425 - acc: 0.7419 - val_loss: 1.0560 - val_acc: 0.7485\n",
      "Epoch 142/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0377 - acc: 0.7537 - val_loss: 1.0501 - val_acc: 0.7840\n",
      "Epoch 143/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0373 - acc: 0.7696 - val_loss: 1.0553 - val_acc: 0.7248\n",
      "Epoch 144/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.0341 - acc: 0.7654 - val_loss: 1.0510 - val_acc: 0.7497\n",
      "Epoch 145/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.0334 - acc: 0.7511 - val_loss: 1.0603 - val_acc: 0.6850\n",
      "Epoch 146/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.0331 - acc: 0.7352 - val_loss: 1.0424 - val_acc: 0.7334\n",
      "Epoch 147/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.0241 - acc: 0.7445 - val_loss: 1.0331 - val_acc: 0.7276\n",
      "Epoch 148/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0198 - acc: 0.7564 - val_loss: 1.0301 - val_acc: 0.7455\n",
      "Epoch 149/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0165 - acc: 0.7575 - val_loss: 1.0305 - val_acc: 0.7297\n",
      "Epoch 150/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0130 - acc: 0.7714 - val_loss: 1.0278 - val_acc: 0.7656\n",
      "Epoch 151/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0117 - acc: 0.7674 - val_loss: 1.0330 - val_acc: 0.7193\n",
      "Epoch 152/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0127 - acc: 0.7476 - val_loss: 1.0295 - val_acc: 0.7583\n",
      "Epoch 153/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0114 - acc: 0.7657 - val_loss: 1.0289 - val_acc: 0.7487\n",
      "Epoch 154/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.0053 - acc: 0.7554 - val_loss: 1.0211 - val_acc: 0.7817\n",
      "Epoch 155/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 1.0091 - acc: 0.7477 - val_loss: 1.0353 - val_acc: 0.7197\n",
      "Epoch 156/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.0067 - acc: 0.7426 - val_loss: 1.0243 - val_acc: 0.7351\n",
      "Epoch 157/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9992 - acc: 0.7710 - val_loss: 1.0187 - val_acc: 0.7642\n",
      "Epoch 158/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 1.0012 - acc: 0.7459 - val_loss: 1.0161 - val_acc: 0.7319\n",
      "Epoch 159/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 0.9966 - acc: 0.7451 - val_loss: 1.0078 - val_acc: 0.7378\n",
      "Epoch 160/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 0.9889 - acc: 0.7580 - val_loss: 1.0034 - val_acc: 0.7744\n",
      "Epoch 161/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 0.9860 - acc: 0.7752 - val_loss: 1.0056 - val_acc: 0.7788\n",
      "Epoch 162/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 0.9880 - acc: 0.7595 - val_loss: 0.9986 - val_acc: 0.7947\n",
      "Epoch 163/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 0.9875 - acc: 0.7552 - val_loss: 0.9952 - val_acc: 0.7629\n",
      "Epoch 164/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9792 - acc: 0.7610 - val_loss: 0.9937 - val_acc: 0.7504\n",
      "Epoch 165/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9750 - acc: 0.7727 - val_loss: 0.9915 - val_acc: 0.7522\n",
      "Epoch 166/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9762 - acc: 0.7683 - val_loss: 0.9914 - val_acc: 0.7812\n",
      "Epoch 167/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9738 - acc: 0.7642 - val_loss: 0.9846 - val_acc: 0.7850\n",
      "Epoch 168/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9708 - acc: 0.7742 - val_loss: 0.9885 - val_acc: 0.7419\n",
      "Epoch 169/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9706 - acc: 0.7617 - val_loss: 0.9825 - val_acc: 0.7548\n",
      "Epoch 170/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9674 - acc: 0.7696 - val_loss: 0.9840 - val_acc: 0.7789\n",
      "Epoch 171/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9632 - acc: 0.7867 - val_loss: 0.9766 - val_acc: 0.7698\n",
      "Epoch 172/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9582 - acc: 0.7917 - val_loss: 0.9795 - val_acc: 0.7814\n",
      "Epoch 173/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9608 - acc: 0.7851 - val_loss: 0.9810 - val_acc: 0.8000\n",
      "Epoch 174/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9564 - acc: 0.8051 - val_loss: 0.9652 - val_acc: 0.8145\n",
      "Epoch 175/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9513 - acc: 0.7932 - val_loss: 0.9772 - val_acc: 0.7311\n",
      "Epoch 176/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9572 - acc: 0.7644 - val_loss: 0.9718 - val_acc: 0.7714\n",
      "Epoch 177/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9532 - acc: 0.7746 - val_loss: 0.9696 - val_acc: 0.7262\n",
      "Epoch 178/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9543 - acc: 0.7584 - val_loss: 0.9650 - val_acc: 0.7809\n",
      "Epoch 179/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9495 - acc: 0.7819 - val_loss: 0.9579 - val_acc: 0.7948\n",
      "Epoch 180/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9445 - acc: 0.8019 - val_loss: 0.9610 - val_acc: 0.7683\n",
      "Epoch 181/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9442 - acc: 0.7858 - val_loss: 0.9592 - val_acc: 0.8007\n",
      "Epoch 182/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9390 - acc: 0.7986 - val_loss: 0.9555 - val_acc: 0.7745\n",
      "Epoch 183/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9397 - acc: 0.7866 - val_loss: 0.9524 - val_acc: 0.8243\n",
      "Epoch 184/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9362 - acc: 0.8004 - val_loss: 0.9443 - val_acc: 0.8223\n",
      "Epoch 185/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9336 - acc: 0.8037 - val_loss: 0.9484 - val_acc: 0.7546\n",
      "Epoch 186/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9339 - acc: 0.7734 - val_loss: 0.9476 - val_acc: 0.7820\n",
      "Epoch 187/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9298 - acc: 0.7936 - val_loss: 0.9420 - val_acc: 0.7967\n",
      "Epoch 188/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9262 - acc: 0.7858 - val_loss: 0.9383 - val_acc: 0.8011\n",
      "Epoch 189/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9263 - acc: 0.7954 - val_loss: 0.9410 - val_acc: 0.8055\n",
      "Epoch 190/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9262 - acc: 0.7892 - val_loss: 0.9333 - val_acc: 0.7776\n",
      "Epoch 191/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9212 - acc: 0.7929 - val_loss: 0.9344 - val_acc: 0.8051\n",
      "Epoch 192/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9183 - acc: 0.8048 - val_loss: 0.9370 - val_acc: 0.7617\n",
      "Epoch 193/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9135 - acc: 0.8065 - val_loss: 0.9265 - val_acc: 0.8082\n",
      "Epoch 194/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9100 - acc: 0.8223 - val_loss: 0.9298 - val_acc: 0.8229\n",
      "Epoch 195/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9124 - acc: 0.7994 - val_loss: 0.9265 - val_acc: 0.7937\n",
      "Epoch 196/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9059 - acc: 0.8314 - val_loss: 0.9236 - val_acc: 0.7816\n",
      "Epoch 197/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9077 - acc: 0.7964 - val_loss: 0.9210 - val_acc: 0.8232\n",
      "Epoch 198/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9083 - acc: 0.7956 - val_loss: 0.9187 - val_acc: 0.8036\n",
      "Epoch 199/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9056 - acc: 0.7969 - val_loss: 0.9188 - val_acc: 0.8158\n",
      "Epoch 200/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9039 - acc: 0.8086 - val_loss: 0.9228 - val_acc: 0.7948\n",
      "Epoch 201/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9036 - acc: 0.8103 - val_loss: 0.9213 - val_acc: 0.7873\n",
      "Epoch 202/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9019 - acc: 0.8062 - val_loss: 0.9182 - val_acc: 0.7765\n",
      "Epoch 203/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.9014 - acc: 0.7975 - val_loss: 0.9176 - val_acc: 0.7779\n",
      "Epoch 204/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8973 - acc: 0.8085 - val_loss: 0.9079 - val_acc: 0.8097\n",
      "Epoch 205/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8938 - acc: 0.8013 - val_loss: 0.9098 - val_acc: 0.8187\n",
      "Epoch 206/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8930 - acc: 0.8214 - val_loss: 0.9106 - val_acc: 0.8167\n",
      "Epoch 207/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8903 - acc: 0.8180 - val_loss: 0.9042 - val_acc: 0.8269\n",
      "Epoch 208/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8843 - acc: 0.8411 - val_loss: 0.8998 - val_acc: 0.8169\n",
      "Epoch 209/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8808 - acc: 0.8353 - val_loss: 0.8975 - val_acc: 0.8329\n",
      "Epoch 210/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8828 - acc: 0.8166 - val_loss: 0.8968 - val_acc: 0.8024\n",
      "Epoch 211/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8813 - acc: 0.8132 - val_loss: 0.8959 - val_acc: 0.7931\n",
      "Epoch 212/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8769 - acc: 0.8245 - val_loss: 0.8890 - val_acc: 0.8344\n",
      "Epoch 213/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8736 - acc: 0.8304 - val_loss: 0.8945 - val_acc: 0.8081\n",
      "Epoch 214/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8758 - acc: 0.8236 - val_loss: 0.8884 - val_acc: 0.8219\n",
      "Epoch 215/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8732 - acc: 0.8266 - val_loss: 0.8923 - val_acc: 0.8230\n",
      "Epoch 216/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8715 - acc: 0.8222 - val_loss: 0.8832 - val_acc: 0.8297\n",
      "Epoch 217/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8690 - acc: 0.8282 - val_loss: 0.8877 - val_acc: 0.7850\n",
      "Epoch 218/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8698 - acc: 0.8147 - val_loss: 0.8866 - val_acc: 0.8460\n",
      "Epoch 219/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8702 - acc: 0.8274 - val_loss: 0.8872 - val_acc: 0.7945\n",
      "Epoch 220/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8692 - acc: 0.8112 - val_loss: 0.8745 - val_acc: 0.8447\n",
      "Epoch 221/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8639 - acc: 0.8266 - val_loss: 0.8797 - val_acc: 0.8024\n",
      "Epoch 222/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8626 - acc: 0.8274 - val_loss: 0.8780 - val_acc: 0.8248\n",
      "Epoch 223/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8615 - acc: 0.8374 - val_loss: 0.8790 - val_acc: 0.8524\n",
      "Epoch 224/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8604 - acc: 0.8383 - val_loss: 0.8733 - val_acc: 0.8203\n",
      "Epoch 225/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8568 - acc: 0.8384 - val_loss: 0.8781 - val_acc: 0.7899\n",
      "Epoch 226/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8561 - acc: 0.8322 - val_loss: 0.8714 - val_acc: 0.8336\n",
      "Epoch 227/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8509 - acc: 0.8434 - val_loss: 0.8656 - val_acc: 0.8038\n",
      "Epoch 228/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8508 - acc: 0.8273 - val_loss: 0.8695 - val_acc: 0.8327\n",
      "Epoch 229/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8492 - acc: 0.8405 - val_loss: 0.8602 - val_acc: 0.8626\n",
      "Epoch 230/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8466 - acc: 0.8480 - val_loss: 0.8624 - val_acc: 0.8223\n",
      "Epoch 231/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8477 - acc: 0.8382 - val_loss: 0.8604 - val_acc: 0.8219\n",
      "Epoch 232/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8481 - acc: 0.8269 - val_loss: 0.8628 - val_acc: 0.8239\n",
      "Epoch 233/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8461 - acc: 0.8326 - val_loss: 0.8601 - val_acc: 0.8207\n",
      "Epoch 234/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8446 - acc: 0.8336 - val_loss: 0.8572 - val_acc: 0.7989\n",
      "Epoch 235/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8430 - acc: 0.8212 - val_loss: 0.8571 - val_acc: 0.8026\n",
      "Epoch 236/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8431 - acc: 0.8255 - val_loss: 0.8600 - val_acc: 0.7831\n",
      "Epoch 237/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8426 - acc: 0.8186 - val_loss: 0.8580 - val_acc: 0.7925\n",
      "Epoch 238/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8364 - acc: 0.8283 - val_loss: 0.8467 - val_acc: 0.8399\n",
      "Epoch 239/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8357 - acc: 0.8391 - val_loss: 0.8472 - val_acc: 0.8413\n",
      "Epoch 240/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8321 - acc: 0.8441 - val_loss: 0.8489 - val_acc: 0.8247\n",
      "Epoch 241/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8346 - acc: 0.8370 - val_loss: 0.8503 - val_acc: 0.8359\n",
      "Epoch 242/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8301 - acc: 0.8385 - val_loss: 0.8435 - val_acc: 0.8468\n",
      "Epoch 243/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8267 - acc: 0.8506 - val_loss: 0.8407 - val_acc: 0.8657\n",
      "Epoch 244/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8285 - acc: 0.8378 - val_loss: 0.8387 - val_acc: 0.8411\n",
      "Epoch 245/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8276 - acc: 0.8267 - val_loss: 0.8413 - val_acc: 0.8397\n",
      "Epoch 246/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8277 - acc: 0.8347 - val_loss: 0.8406 - val_acc: 0.8267\n",
      "Epoch 247/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8241 - acc: 0.8520 - val_loss: 0.8407 - val_acc: 0.8445\n",
      "Epoch 248/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8231 - acc: 0.8340 - val_loss: 0.8389 - val_acc: 0.8223\n",
      "Epoch 249/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8223 - acc: 0.8278 - val_loss: 0.8361 - val_acc: 0.8224\n",
      "Epoch 250/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8199 - acc: 0.8442 - val_loss: 0.8351 - val_acc: 0.8165\n",
      "Epoch 251/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8229 - acc: 0.8252 - val_loss: 0.8368 - val_acc: 0.8117\n",
      "Epoch 252/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8182 - acc: 0.8448 - val_loss: 0.8286 - val_acc: 0.8698\n",
      "Epoch 253/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8128 - acc: 0.8650 - val_loss: 0.8326 - val_acc: 0.8530\n",
      "Epoch 254/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8120 - acc: 0.8523 - val_loss: 0.8250 - val_acc: 0.8625\n",
      "Epoch 255/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8084 - acc: 0.8675 - val_loss: 0.8217 - val_acc: 0.8623\n",
      "Epoch 256/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8067 - acc: 0.8686 - val_loss: 0.8262 - val_acc: 0.8717\n",
      "Epoch 257/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8064 - acc: 0.8559 - val_loss: 0.8246 - val_acc: 0.8590\n",
      "Epoch 258/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8065 - acc: 0.8511 - val_loss: 0.8179 - val_acc: 0.8811\n",
      "Epoch 259/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8028 - acc: 0.8719 - val_loss: 0.8260 - val_acc: 0.8312\n",
      "Epoch 260/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8043 - acc: 0.8538 - val_loss: 0.8265 - val_acc: 0.8292\n",
      "Epoch 261/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8018 - acc: 0.8523 - val_loss: 0.8212 - val_acc: 0.8356\n",
      "Epoch 262/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8025 - acc: 0.8510 - val_loss: 0.8222 - val_acc: 0.8476\n",
      "Epoch 263/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8025 - acc: 0.8491 - val_loss: 0.8151 - val_acc: 0.8377\n",
      "Epoch 264/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8012 - acc: 0.8527 - val_loss: 0.8150 - val_acc: 0.8120\n",
      "Epoch 265/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.8002 - acc: 0.8387 - val_loss: 0.8166 - val_acc: 0.8486\n",
      "Epoch 266/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7973 - acc: 0.8576 - val_loss: 0.8102 - val_acc: 0.8584\n",
      "Epoch 267/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7946 - acc: 0.8635 - val_loss: 0.8102 - val_acc: 0.8510\n",
      "Epoch 268/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7955 - acc: 0.8449 - val_loss: 0.8118 - val_acc: 0.8208\n",
      "Epoch 269/300\n",
      "75371/75371 [==============================] - 0s 3us/step - loss: 0.7957 - acc: 0.8385 - val_loss: 0.8081 - val_acc: 0.8426\n",
      "Epoch 270/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7946 - acc: 0.8440 - val_loss: 0.8143 - val_acc: 0.8068\n",
      "Epoch 271/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7936 - acc: 0.8454 - val_loss: 0.8106 - val_acc: 0.8848\n",
      "Epoch 272/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7935 - acc: 0.8626 - val_loss: 0.8006 - val_acc: 0.8533\n",
      "Epoch 273/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7867 - acc: 0.8654 - val_loss: 0.7991 - val_acc: 0.8566\n",
      "Epoch 274/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7857 - acc: 0.8580 - val_loss: 0.8057 - val_acc: 0.8402\n",
      "Epoch 275/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7890 - acc: 0.8580 - val_loss: 0.8074 - val_acc: 0.8671\n",
      "Epoch 276/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7868 - acc: 0.8740 - val_loss: 0.8032 - val_acc: 0.8602\n",
      "Epoch 277/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7850 - acc: 0.8591 - val_loss: 0.7986 - val_acc: 0.8526\n",
      "Epoch 278/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7818 - acc: 0.8508 - val_loss: 0.8011 - val_acc: 0.8236\n",
      "Epoch 279/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7836 - acc: 0.8427 - val_loss: 0.8011 - val_acc: 0.8378\n",
      "Epoch 280/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7834 - acc: 0.8473 - val_loss: 0.7973 - val_acc: 0.8560\n",
      "Epoch 281/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7803 - acc: 0.8545 - val_loss: 0.7925 - val_acc: 0.8690\n",
      "Epoch 282/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7746 - acc: 0.8724 - val_loss: 0.7926 - val_acc: 0.8613\n",
      "Epoch 283/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7748 - acc: 0.8714 - val_loss: 0.7900 - val_acc: 0.8662\n",
      "Epoch 284/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7743 - acc: 0.8613 - val_loss: 0.7890 - val_acc: 0.8399\n",
      "Epoch 285/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7719 - acc: 0.8624 - val_loss: 0.7948 - val_acc: 0.8505\n",
      "Epoch 286/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7739 - acc: 0.8605 - val_loss: 0.7909 - val_acc: 0.8341\n",
      "Epoch 287/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7703 - acc: 0.8587 - val_loss: 0.7883 - val_acc: 0.8668\n",
      "Epoch 288/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7689 - acc: 0.8711 - val_loss: 0.7883 - val_acc: 0.8273\n",
      "Epoch 289/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7693 - acc: 0.8514 - val_loss: 0.7806 - val_acc: 0.8471\n",
      "Epoch 290/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7664 - acc: 0.8622 - val_loss: 0.7830 - val_acc: 0.8336\n",
      "Epoch 291/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7648 - acc: 0.8651 - val_loss: 0.7820 - val_acc: 0.8676\n",
      "Epoch 292/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7634 - acc: 0.8749 - val_loss: 0.7767 - val_acc: 0.8582\n",
      "Epoch 293/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7628 - acc: 0.8655 - val_loss: 0.7755 - val_acc: 0.8583\n",
      "Epoch 294/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7597 - acc: 0.8738 - val_loss: 0.7790 - val_acc: 0.8454\n",
      "Epoch 295/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7598 - acc: 0.8674 - val_loss: 0.7748 - val_acc: 0.8627\n",
      "Epoch 296/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7580 - acc: 0.8809 - val_loss: 0.7817 - val_acc: 0.8663\n",
      "Epoch 297/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7614 - acc: 0.8815 - val_loss: 0.7814 - val_acc: 0.8724\n",
      "Epoch 298/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7586 - acc: 0.8820 - val_loss: 0.7748 - val_acc: 0.8813\n",
      "Epoch 299/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7546 - acc: 0.8837 - val_loss: 0.7714 - val_acc: 0.8490\n",
      "Epoch 300/300\n",
      "75371/75371 [==============================] - 0s 4us/step - loss: 0.7554 - acc: 0.8776 - val_loss: 0.7687 - val_acc: 0.8778\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10000\n",
    "epochs = 300\n",
    "\n",
    "history = model.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Visualize training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train(hist):\n",
    "    h = hist.history\n",
    "    if 'acc' in h:\n",
    "        meas='acc'\n",
    "        loc='lower right'\n",
    "    else:\n",
    "        meas='loss'\n",
    "        loc='upper right'\n",
    "    plt.plot(hist.history[meas])\n",
    "    plt.plot(hist.history['val_'+meas])\n",
    "    plt.title('model '+meas)\n",
    "    plt.ylabel(meas)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc=loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cb6746ebe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_train(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Evaluate & save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18843/18843 [==============================] - 1s 50us/step\n",
      "Test Accuracy:  0.877779546769\n",
      "Test Loss:  0.76873274958\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test,y_test,verbose=1)\n",
    "print('Test Accuracy: ',score[1])\n",
    "print('Test Loss: ',score[0])\n",
    "\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Let's make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  87.59\n",
      "(Challenge) Prediction Accuracy:  87.7220227118315\n"
     ]
    }
   ],
   "source": [
    "num_test = 10000\n",
    "count = 0\n",
    "correct = 0\n",
    "\n",
    "# Random test\n",
    "for i in range(num_test):\n",
    "    count = count + 1\n",
    "    d1 = np.random.randint(0,num_of_class_x)\n",
    "    d2 = np.random.randint(0,num_of_class_y)\n",
    "    d3 = np.array([[d1,d2]]).astype('float32') / (num_of_class_x - 1)    \n",
    "    d4 = np.argmax(model.predict(d3))\n",
    "    if d4 == d1 + d2:\n",
    "        correct = correct + 1\n",
    "print('Prediction Accuracy: ',correct/count*100)\n",
    "\n",
    "# Challenging test (with removed data)\n",
    "for i in range(num_of_class_x):\n",
    "    for j in remove_list:\n",
    "        count = count + 1\n",
    "        d1 = np.random.randint(0,num_of_class_x)\n",
    "        d2 = j\n",
    "        d3 = np.array([[d1,d2]]).astype('float32') / (num_of_class_x - 1)    \n",
    "        d4 = np.argmax(model.predict(d3))\n",
    "        if d4 == d1 + d2:\n",
    "            correct = correct + 1\n",
    "print('(Challenge) Prediction Accuracy: ',correct/count*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
