{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 5: Personal Project 2 - Industrial Defect Data Generation with GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import initializers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, Conv2DTranspose\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers.core import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import math\n",
    "import glob\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "img_list = []\n",
    "\n",
    "for img in glob.glob('crop/*.PNG'):\n",
    "    input_img = cv2.imread(img,0)\n",
    "    input_img = cv2.resize(input_img,(28,28))\n",
    "    img_list.append(input_img)\n",
    "\n",
    "X_train = np.array(img_list)\n",
    "X_train = (X_train.reshape(-1, 28, 28, 1).astype(np.float32)-127.5) / 127.5\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(7*7*50, input_dim=100))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Reshape((7, 7, 50)))\n",
    "    model.add(Conv2DTranspose(20, (5,5), strides=2, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Conv2DTranspose(1, (5,5), strides=2, padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(20, (5, 5), strides=2, padding='same', input_shape=(28, 28, 1)))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Conv2D(50, (5, 5), strides=2, padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_containing_discriminator(generator, discriminator):\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    d.trainable = False\n",
    "    model.add(discriminator)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 20)        520       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 14, 14, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 7, 7, 50)          25050     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 7, 7, 50)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2450)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 500)               1225500   \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 501       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,251,571\n",
      "Trainable params: 1,251,571\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 2450)              247450    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 2450)              9800      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 2450)              0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 7, 7, 50)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 14, 14, 20)        25020     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 14, 14, 20)        80        \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 14, 14, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 28, 28, 1)         501       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 282,851\n",
      "Trainable params: 277,911\n",
      "Non-trainable params: 4,940\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "d = discriminator_model()\n",
    "g = generator_model()\n",
    "d_on_g = generator_containing_discriminator(g, d)\n",
    "d_optim = SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
    "g_optim = SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
    "g.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "d_on_g.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "d.trainable = True\n",
    "d.compile(loss='binary_crossentropy', optimizer=d_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num)/width))\n",
    "    shape = generated_images.shape[1:3]\n",
    "    image = np.zeros((height*shape[0], width*shape[1]),\n",
    "                     dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
    "            img[:, :, 0]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch is 0\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.716234\n",
      "batch 0 g_loss : 0.697149\n",
      "Epoch is 1\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.712778\n",
      "batch 0 g_loss : 0.694217\n",
      "Epoch is 2\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.709450\n",
      "batch 0 g_loss : 0.696908\n",
      "Epoch is 3\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.704695\n",
      "batch 0 g_loss : 0.696913\n",
      "Epoch is 4\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.699044\n",
      "batch 0 g_loss : 0.697830\n",
      "Epoch is 5\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.693754\n",
      "batch 0 g_loss : 0.693129\n",
      "Epoch is 6\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.685840\n",
      "batch 0 g_loss : 0.691498\n",
      "Epoch is 7\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.678837\n",
      "batch 0 g_loss : 0.696567\n",
      "Epoch is 8\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.672853\n",
      "batch 0 g_loss : 0.692865\n",
      "Epoch is 9\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.664802\n",
      "batch 0 g_loss : 0.692405\n",
      "Epoch is 10\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.656459\n",
      "batch 0 g_loss : 0.700080\n",
      "Epoch is 11\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.647738\n",
      "batch 0 g_loss : 0.697344\n",
      "Epoch is 12\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.641011\n",
      "batch 0 g_loss : 0.699561\n",
      "Epoch is 13\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.632146\n",
      "batch 0 g_loss : 0.699413\n",
      "Epoch is 14\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.625597\n",
      "batch 0 g_loss : 0.698490\n",
      "Epoch is 15\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.617905\n",
      "batch 0 g_loss : 0.696030\n",
      "Epoch is 16\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.609659\n",
      "batch 0 g_loss : 0.694002\n",
      "Epoch is 17\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.603208\n",
      "batch 0 g_loss : 0.695214\n",
      "Epoch is 18\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.595615\n",
      "batch 0 g_loss : 0.695236\n",
      "Epoch is 19\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.589385\n",
      "batch 0 g_loss : 0.701789\n",
      "Epoch is 20\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.583138\n",
      "batch 0 g_loss : 0.694880\n",
      "Epoch is 21\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.578039\n",
      "batch 0 g_loss : 0.691721\n",
      "Epoch is 22\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.571598\n",
      "batch 0 g_loss : 0.696644\n",
      "Epoch is 23\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.567413\n",
      "batch 0 g_loss : 0.694609\n",
      "Epoch is 24\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.561820\n",
      "batch 0 g_loss : 0.689622\n",
      "Epoch is 25\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.560152\n",
      "batch 0 g_loss : 0.686766\n",
      "Epoch is 26\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.556120\n",
      "batch 0 g_loss : 0.679473\n",
      "Epoch is 27\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.553088\n",
      "batch 0 g_loss : 0.676862\n",
      "Epoch is 28\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.550993\n",
      "batch 0 g_loss : 0.676574\n",
      "Epoch is 29\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.551304\n",
      "batch 0 g_loss : 0.674601\n",
      "Epoch is 30\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.549312\n",
      "batch 0 g_loss : 0.666438\n",
      "Epoch is 31\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.549664\n",
      "batch 0 g_loss : 0.664863\n",
      "Epoch is 32\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.549396\n",
      "batch 0 g_loss : 0.663476\n",
      "Epoch is 33\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.550173\n",
      "batch 0 g_loss : 0.655717\n",
      "Epoch is 34\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.549956\n",
      "batch 0 g_loss : 0.653480\n",
      "Epoch is 35\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.549572\n",
      "batch 0 g_loss : 0.645271\n",
      "Epoch is 36\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.552140\n",
      "batch 0 g_loss : 0.646586\n",
      "Epoch is 37\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.552723\n",
      "batch 0 g_loss : 0.646618\n",
      "Epoch is 38\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.553838\n",
      "batch 0 g_loss : 0.641450\n",
      "Epoch is 39\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.554440\n",
      "batch 0 g_loss : 0.640226\n",
      "Epoch is 40\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.550908\n",
      "batch 0 g_loss : 0.644850\n",
      "Epoch is 41\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.551453\n",
      "batch 0 g_loss : 0.650111\n",
      "Epoch is 42\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.548350\n",
      "batch 0 g_loss : 0.650869\n",
      "Epoch is 43\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.544356\n",
      "batch 0 g_loss : 0.653869\n",
      "Epoch is 44\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.538323\n",
      "batch 0 g_loss : 0.666704\n",
      "Epoch is 45\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.531431\n",
      "batch 0 g_loss : 0.687105\n",
      "Epoch is 46\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.524180\n",
      "batch 0 g_loss : 0.695019\n",
      "Epoch is 47\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.514947\n",
      "batch 0 g_loss : 0.711167\n",
      "Epoch is 48\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.504363\n",
      "batch 0 g_loss : 0.739160\n",
      "Epoch is 49\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.493874\n",
      "batch 0 g_loss : 0.753436\n",
      "Epoch is 50\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.480980\n",
      "batch 0 g_loss : 0.775949\n",
      "Epoch is 51\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.467342\n",
      "batch 0 g_loss : 0.801275\n",
      "Epoch is 52\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.451122\n",
      "batch 0 g_loss : 0.833351\n",
      "Epoch is 53\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.437174\n",
      "batch 0 g_loss : 0.871839\n",
      "Epoch is 54\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.421680\n",
      "batch 0 g_loss : 0.902035\n",
      "Epoch is 55\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.406400\n",
      "batch 0 g_loss : 0.941341\n",
      "Epoch is 56\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.389412\n",
      "batch 0 g_loss : 0.972992\n",
      "Epoch is 57\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.376463\n",
      "batch 0 g_loss : 1.023891\n",
      "Epoch is 58\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.361413\n",
      "batch 0 g_loss : 1.064821\n",
      "Epoch is 59\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.347436\n",
      "batch 0 g_loss : 1.113300\n",
      "Epoch is 60\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.334842\n",
      "batch 0 g_loss : 1.152942\n",
      "Epoch is 61\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.321494\n",
      "batch 0 g_loss : 1.195724\n",
      "Epoch is 62\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.312359\n",
      "batch 0 g_loss : 1.238417\n",
      "Epoch is 63\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.302025\n",
      "batch 0 g_loss : 1.275296\n",
      "Epoch is 64\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.294221\n",
      "batch 0 g_loss : 1.289076\n",
      "Epoch is 65\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.286828\n",
      "batch 0 g_loss : 1.324388\n",
      "Epoch is 66\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.281180\n",
      "batch 0 g_loss : 1.339889\n",
      "Epoch is 67\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.278637\n",
      "batch 0 g_loss : 1.350600\n",
      "Epoch is 68\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.277022\n",
      "batch 0 g_loss : 1.347301\n",
      "Epoch is 69\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.276774\n",
      "batch 0 g_loss : 1.320811\n",
      "Epoch is 70\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.279385\n",
      "batch 0 g_loss : 1.308750\n",
      "Epoch is 71\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.285411\n",
      "batch 0 g_loss : 1.275300\n",
      "Epoch is 72\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.295561\n",
      "batch 0 g_loss : 1.200436\n",
      "Epoch is 73\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.306390\n",
      "batch 0 g_loss : 1.169771\n",
      "Epoch is 74\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.325170\n",
      "batch 0 g_loss : 1.078487\n",
      "Epoch is 75\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.348985\n",
      "batch 0 g_loss : 1.002909\n",
      "Epoch is 76\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.381358\n",
      "batch 0 g_loss : 0.913784\n",
      "Epoch is 77\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.416871\n",
      "batch 0 g_loss : 0.825614\n",
      "Epoch is 78\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.462734\n",
      "batch 0 g_loss : 0.745433\n",
      "Epoch is 79\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.510959\n",
      "batch 0 g_loss : 0.683479\n",
      "Epoch is 80\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.562965\n",
      "batch 0 g_loss : 0.614268\n",
      "Epoch is 81\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.600864\n",
      "batch 0 g_loss : 0.566384\n",
      "Epoch is 82\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.634418\n",
      "batch 0 g_loss : 0.530703\n",
      "Epoch is 83\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.651497\n",
      "batch 0 g_loss : 0.526953\n",
      "Epoch is 84\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.658432\n",
      "batch 0 g_loss : 0.524104\n",
      "Epoch is 85\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.637943\n",
      "batch 0 g_loss : 0.564603\n",
      "Epoch is 86\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.599686\n",
      "batch 0 g_loss : 0.601836\n",
      "Epoch is 87\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.540724\n",
      "batch 0 g_loss : 0.685154\n",
      "Epoch is 88\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.474508\n",
      "batch 0 g_loss : 0.787354\n",
      "Epoch is 89\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.401898\n",
      "batch 0 g_loss : 0.915951\n",
      "Epoch is 90\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.331446\n",
      "batch 0 g_loss : 1.065819\n",
      "Epoch is 91\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.268457\n",
      "batch 0 g_loss : 1.253587\n",
      "Epoch is 92\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.212690\n",
      "batch 0 g_loss : 1.467418\n",
      "Epoch is 93\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.167298\n",
      "batch 0 g_loss : 1.691192\n",
      "Epoch is 94\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.133455\n",
      "batch 0 g_loss : 1.890397\n",
      "Epoch is 95\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.108406\n",
      "batch 0 g_loss : 2.127020\n",
      "Epoch is 96\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.089769\n",
      "batch 0 g_loss : 2.325142\n",
      "Epoch is 97\n",
      "Number of batches 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 d_loss : 0.076253\n",
      "batch 0 g_loss : 2.498605\n",
      "Epoch is 98\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.066180\n",
      "batch 0 g_loss : 2.682301\n",
      "Epoch is 99\n",
      "Number of batches 1\n",
      "batch 0 d_loss : 0.058989\n",
      "batch 0 g_loss : 2.841272\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "for epoch in range(100):\n",
    "    print(\"Epoch is\", epoch)\n",
    "    print(\"Number of batches\", int(X_train.shape[0]/BATCH_SIZE))\n",
    "    for index in range(int(X_train.shape[0]/BATCH_SIZE)):\n",
    "        noise = np.random.uniform(-1, 1, size=(BATCH_SIZE, 100))\n",
    "        image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
    "        generated_images = g.predict(noise, verbose=0)\n",
    "        if index % 20 == 0:\n",
    "            image = combine_images(generated_images)\n",
    "            image = image*127.5+127.5\n",
    "            X = np.concatenate((image_batch, generated_images))\n",
    "            y = [1] * BATCH_SIZE + [0] * BATCH_SIZE\n",
    "            d_loss = d.train_on_batch(X, y)\n",
    "            print(\"batch %d d_loss : %f\" % (index, d_loss))\n",
    "            noise = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n",
    "            d.trainable = False\n",
    "            g_loss = d_on_g.train_on_batch(noise, [1] * BATCH_SIZE)\n",
    "            d.trainable = True\n",
    "            print(\"batch %d g_loss : %f\" % (index, g_loss))\n",
    "            if index % 10 == 9:\n",
    "                g.save_weights('generator', True)\n",
    "                d.save_weights('discriminator', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(BATCH_SIZE, nice=False):\n",
    "    g = generator_model()\n",
    "    g.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "    g.load_weights('generator')\n",
    "    if nice:\n",
    "        d = discriminator_model()\n",
    "        d.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "        d.load_weights('discriminator')\n",
    "        noise = np.random.uniform(-1, 1, (BATCH_SIZE*20, 100))\n",
    "        generated_images = g.predict(noise, verbose=1)\n",
    "        d_pret = d.predict(generated_images, verbose=1)\n",
    "        index = np.arange(0, BATCH_SIZE*20)\n",
    "        index.resize((BATCH_SIZE*20, 1))\n",
    "        pre_with_index = list(np.append(d_pret, index, axis=1))\n",
    "        pre_with_index.sort(key=lambda x: x[0], reverse=True)\n",
    "        nice_images = np.zeros((BATCH_SIZE,) + generated_images.shape[1:3], dtype=np.float32)\n",
    "        nice_images = nice_images[:, :, :, None]\n",
    "        for i in range(BATCH_SIZE):\n",
    "            idx = int(pre_with_index[i][1])\n",
    "            nice_images[i, :, :, 0] = generated_images[idx, :, :, 0]\n",
    "        image = combine_images(nice_images)\n",
    "    else:\n",
    "        noise = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n",
    "        generated_images = g.predict(noise, verbose=1)\n",
    "        image = combine_images(generated_images)\n",
    "    image = image*127.5+127.5\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\n",
    "        \"generated_image.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
